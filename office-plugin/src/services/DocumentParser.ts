/**
 * DocumentParser - æ–‡æ¡£è§£ææœåŠ¡
 * æ”¯æŒè§£æå¤šç§æ–‡æ¡£æ ¼å¼å¹¶æå–æ–‡æœ¬å†…å®¹
 * 
 * æ”¯æŒçš„æ ¼å¼ï¼š
 * - PDF (.pdf) - ä½¿ç”¨ CDN åŠ¨æ€åŠ è½½ pdf.js
 * - Word (.docx) - ä½¿ç”¨ JSZip è§£æ XML å†…å®¹ï¼ˆæµè§ˆå™¨å…¼å®¹ï¼‰
 * - Excel (.xlsx, .xls, .csv) - ä½¿ç”¨åŠ¨æ€åŠ è½½çš„ xlsx
 * - æ–‡æœ¬æ–‡ä»¶ (.txt, .md, .json, .xml, .html, .htm)
 * - ä»£ç æ–‡ä»¶ (.js, .ts, .py, .java, .c, .cpp, .go, .rs, .sql ç­‰)
 */

import Logger from '../utils/logger'

const logger = new Logger('DocumentParser')

// ç¼“å­˜å·²åŠ è½½çš„åº“
let pdfjsLib: unknown = null
let XLSX: unknown = null
let JSZip: unknown = null

// PDF.js CDN ç‰ˆæœ¬
const PDFJS_VERSION = '4.4.168'
const PDFJS_CDN_URL = `https://cdnjs.cloudflare.com/ajax/libs/pdf.js/${PDFJS_VERSION}/pdf.min.mjs`
const PDFJS_WORKER_URL = `https://cdnjs.cloudflare.com/ajax/libs/pdf.js/${PDFJS_VERSION}/pdf.worker.min.mjs`

// ä» CDN åŠ è½½ PDF.js
async function loadPdfJs(): Promise<any> {
  if (pdfjsLib) return pdfjsLib
  
  try {
    // ä½¿ç”¨åŠ¨æ€ import ä» CDN åŠ è½½
    pdfjsLib = await import(/* @vite-ignore */ PDFJS_CDN_URL)
    ;(pdfjsLib as { GlobalWorkerOptions: { workerSrc: string } }).GlobalWorkerOptions.workerSrc = PDFJS_WORKER_URL
    logger.info('PDF.js loaded from CDN')
    return pdfjsLib
  } catch (error) {
    logger.error('Failed to load PDF.js from CDN', error instanceof Error ? error : { error })
    throw new Error('æ— æ³•åŠ è½½ PDF è§£æåº“ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥')
  }
}

async function loadXLSX(): Promise<typeof import('xlsx')> {
  if (!XLSX) {
    XLSX = await import('xlsx')
  }
  return XLSX as typeof import('xlsx')
}

async function loadJSZip(): Promise<unknown> {
  if (!JSZip) {
    JSZip = await import('jszip').then(m => m.default)
  }
  return JSZip
}

/** æ–‡æ¡£ç»“æ„å…ƒç´  */
export interface DocumentSection {
  type: 'heading' | 'paragraph' | 'list' | 'table' | 'code' | 'image'
  level?: number  // æ ‡é¢˜çº§åˆ« 1-6
  content: string
  startIndex?: number  // åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
}

/** æ–‡æ¡£å…ƒæ•°æ® */
export interface DocumentMetadata {
  title?: string
  author?: string
  createdAt?: string
  modifiedAt?: string
  subject?: string
  keywords?: string[]
}

/** æ–‡æ¡£æ‘˜è¦ */
export interface DocumentSummary {
  brief: string  // ç®€çŸ­æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰
  keyPoints: string[]  // å…³é”®ç‚¹åˆ—è¡¨
  tableOfContents?: string[]  // ç›®å½•ç»“æ„
}

export interface ParseResult {
  success: boolean
  text: string
  error?: string
  pageCount?: number
  wordCount?: number
  sheetCount?: number  // Excel å·¥ä½œè¡¨æ•°é‡
  slideCount?: number  // PPT å¹»ç¯ç‰‡æ•°é‡
  /** ç»“æ„åŒ–å†…å®¹ï¼ˆç« èŠ‚ã€æ®µè½ç­‰ï¼‰ */
  sections?: DocumentSection[]
  /** æ–‡æ¡£å…ƒæ•°æ® */
  metadata?: DocumentMetadata
  /** è‡ªåŠ¨ç”Ÿæˆçš„æ‘˜è¦ */
  summary?: DocumentSummary
  /** æ£€æµ‹åˆ°çš„å†…å®¹ç±»å‹ */
  contentType?: 'document' | 'spreadsheet' | 'presentation' | 'code' | 'data'
  /** æ–‡ä»¶è¯­è¨€ï¼ˆä»£ç æ–‡ä»¶ï¼‰ */
  language?: string
}

// æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
const TEXT_EXTENSIONS = [
  '.txt', '.md', '.markdown', '.rst', '.log',
  '.json', '.xml', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
  '.html', '.htm', '.css', '.scss', '.less', '.sass',
  '.js', '.ts', '.jsx', '.tsx', '.mjs', '.cjs',
  '.py', '.pyw', '.pyi',
  '.java', '.kt', '.kts', '.groovy', '.scala',
  '.c', '.h', '.cpp', '.hpp', '.cc', '.cxx',
  '.cs', '.fs', '.vb',
  '.go', '.rs', '.swift', '.m', '.mm',
  '.rb', '.php', '.pl', '.pm', '.lua',
  '.r', '.R', '.rmd', '.Rmd',
  '.sql', '.sh', '.bash', '.zsh', '.ps1', '.bat', '.cmd',
  '.dockerfile', '.gitignore', '.env', '.editorconfig'
]

const CODE_EXTENSIONS = [
  '.js', '.ts', '.jsx', '.tsx', '.mjs', '.cjs',
  '.py', '.pyw', '.pyi',
  '.java', '.kt', '.kts', '.groovy', '.scala',
  '.c', '.h', '.cpp', '.hpp', '.cc', '.cxx',
  '.cs', '.fs', '.vb',
  '.go', '.rs', '.swift', '.m', '.mm',
  '.rb', '.php', '.pl', '.pm', '.lua',
  '.r', '.R', '.sql', '.sh', '.bash', '.zsh', '.ps1', '.bat', '.cmd'
]

class DocumentParserClass {
  constructor() {
    // æ„é€ å‡½æ•°ä¸ºç©ºï¼Œåº“æŒ‰éœ€åŠ¨æ€åŠ è½½
  }

  /**
   * è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨
   */
  getSupportedFormats(): string {
    return `
æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼š
â€¢ PDF æ–‡æ¡£ (.pdf)
â€¢ Word æ–‡æ¡£ (.docx, .doc)
â€¢ Excel è¡¨æ ¼ (.xlsx, .xls, .csv)
â€¢ PowerPoint æ¼”ç¤ºæ–‡ç¨¿ (.pptx)
â€¢ æ–‡æœ¬æ–‡ä»¶ (.txt, .md, .json, .xml, .html, .yaml ç­‰)
â€¢ ä»£ç æ–‡ä»¶ (.js, .ts, .py, .java, .go, .rs ç­‰)
    `.trim()
  }

  /**
   * æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒè§£æ
   */
  isSupported(fileName: string): boolean {
    const ext = this.getExtension(fileName)
    return this.isSupportedExtension(ext)
  }

  /**
   * è§£ææ–‡æ¡£å¹¶æå–æ–‡æœ¬
   */
  async parse(file: File): Promise<ParseResult> {
    const fileName = file.name.toLowerCase()
    const ext = this.getExtension(fileName)

    logger.info('å¼€å§‹è§£ææ–‡æ¡£', { fileName, fileType: file.type })

    try {
      // PDF æ–‡æ¡£
      if (ext === '.pdf') {
        return await this.parsePDF(file)
      }

      // Word æ–‡æ¡£ (.docx)
      if (ext === '.docx') {
        return await this.parseDocx(file)
      }

      // æ—§ç‰ˆ Word æ–‡æ¡£ (.doc)
      if (ext === '.doc') {
        return await this.parseDoc(file)
      }

      // Excel æ–‡æ¡£
      if (['.xlsx', '.xls'].includes(ext)) {
        return await this.parseExcel(file)
      }

      // CSV æ–‡ä»¶
      if (ext === '.csv') {
        return await this.parseCSV(file)
      }

      // PowerPoint æ–‡æ¡£
      if (ext === '.pptx') {
        return await this.parsePPTX(file)
      }

      // æ–‡æœ¬/ä»£ç æ–‡ä»¶
      if (this.isTextFile(ext)) {
        return await this.parseText(file, CODE_EXTENSIONS.includes(ext))
      }

      // å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
      if (file.type.startsWith('text/')) {
        return await this.parseText(file, false)
      }

      return {
        success: false,
        text: '',
        error: `ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: ${ext || file.type}`
      }
    } catch (error) {
      logger.error('è§£æå¤±è´¥', error instanceof Error ? error : { error })
      return {
        success: false,
        text: '',
        error: error instanceof Error ? error.message : 'è§£æå¤±è´¥'
      }
    }
  }

  /**
   * è§£æ PDF æ–‡æ¡£
   */
  private async parsePDF(file: File): Promise<ParseResult> {
    try {
      const pdfjs = await loadPdfJs()
      const arrayBuffer = await file.arrayBuffer()
      const pdf = await pdfjs.getDocument({ data: arrayBuffer }).promise

      const textParts: string[] = []
      const pageCount = pdf.numPages

      for (let i = 1; i <= pageCount; i++) {
        const page = await pdf.getPage(i)
        const textContent = await page.getTextContent()
        const pageText = textContent.items
          .map((item: any) => item.str)
          .join(' ')
        textParts.push(pageText)
      }

      const fullText = textParts.join('\n\n')
      const wordCount = this.countWords(fullText)

      logger.info('PDF è§£æå®Œæˆ', { pageCount, wordCount })

      return {
        success: true,
        text: fullText,
        pageCount,
        wordCount
      }
    } catch (error) {
      logger.error('PDF è§£æå¤±è´¥', error instanceof Error ? error : { error })
      return {
        success: false,
        text: '',
        error: `PDF è§£æå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }
  }

  /**
   * è§£æ Word æ–‡æ¡£ (.docx)
   * ä½¿ç”¨ JSZip è§£å‹å¹¶æå– XML ä¸­çš„æ–‡æœ¬å†…å®¹ï¼ˆæµè§ˆå™¨å…¼å®¹ï¼‰
   */
  private async parseDocx(file: File): Promise<ParseResult> {
    try {
      const arrayBuffer = await file.arrayBuffer()
      const zip = await this.unzip(arrayBuffer)
      
      // docx çš„ä¸»æ–‡æ¡£å†…å®¹åœ¨ word/document.xml ä¸­
      const documentXml = zip['word/document.xml']
      if (!documentXml) {
        throw new Error('æ— æ³•æ‰¾åˆ°æ–‡æ¡£å†…å®¹')
      }

      const text = this.extractTextFromDocx(documentXml)
      const wordCount = this.countWords(text)

      // æå–ç»“æ„åŒ–ä¿¡æ¯
      const sections = this.extractSections(text)
      const summary = this.generateSummary(text, sections)

      // æå–å…ƒæ•°æ®
      const coreXml = zip['docProps/core.xml']
      const metadata = this.extractDocxMetadata(coreXml)

      logger.info('Word (.docx) è§£æå®Œæˆ', { 
        wordCount, 
        sectionCount: sections.length,
        hasMetadata: !!metadata 
      })

      return {
        success: true,
        text,
        wordCount,
        sections,
        summary,
        metadata,
        contentType: 'document'
      }
    } catch (error) {
      logger.error('Word (.docx) è§£æå¤±è´¥', error instanceof Error ? error : { error })
      return {
        success: false,
        text: '',
        error: `Word è§£æå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }
  }

  /**
   * è§£ææ—§ç‰ˆ Word æ–‡æ¡£ (.doc)
   * æ³¨æ„ï¼šæµè§ˆå™¨ç¯å¢ƒä¸æ”¯æŒç›´æ¥è§£æ .doc æ ¼å¼
   */
  private async parseDoc(_file: File): Promise<ParseResult> {
    // .doc æ˜¯ OLE äºŒè¿›åˆ¶æ ¼å¼ï¼Œåœ¨æµè§ˆå™¨ä¸­éš¾ä»¥ç›´æ¥è§£æ
    // å»ºè®®ç”¨æˆ·è½¬æ¢ä¸º .docx æ ¼å¼
    return {
      success: false,
      text: '',
      error: 'æµè§ˆå™¨ä¸æ”¯æŒç›´æ¥è§£æ .doc æ ¼å¼ã€‚è¯·å°†æ–‡ä»¶å¦å­˜ä¸º .docx æ ¼å¼åé‡æ–°ä¸Šä¼ ã€‚'
    }
  }

  /**
   * è§£æ Excel æ–‡æ¡£
   */
  private async parseExcel(file: File): Promise<ParseResult> {
    try {
      const xlsxLib = await loadXLSX()
      const arrayBuffer = await file.arrayBuffer()
      const workbook = xlsxLib.read(arrayBuffer, { type: 'array' })

      const textParts: string[] = []
      const sheetCount = workbook.SheetNames.length

      for (const sheetName of workbook.SheetNames) {
        const worksheet = workbook.Sheets[sheetName]
        const csv = xlsxLib.utils.sheet_to_csv(worksheet)
        textParts.push(`=== å·¥ä½œè¡¨: ${sheetName} ===\n${csv}`)
      }

      const fullText = textParts.join('\n\n')
      const wordCount = this.countWords(fullText)

      logger.info('Excel è§£æå®Œæˆ', { sheetCount, wordCount })

      return {
        success: true,
        text: fullText,
        sheetCount,
        wordCount
      }
    } catch (error) {
      logger.error('Excel è§£æå¤±è´¥', error instanceof Error ? error : { error })
      return {
        success: false,
        text: '',
        error: `Excel è§£æå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }
  }

  /**
   * è§£æ CSV æ–‡ä»¶
   */
  private async parseCSV(file: File): Promise<ParseResult> {
    const text = await file.text()
    const wordCount = this.countWords(text)

    // è®¡ç®—è¡Œæ•°ä½œä¸ºå¤§æ¦‚çš„è®°å½•æ•°
    const lineCount = text.split('\n').filter(line => line.trim()).length

    logger.info('CSV è§£æå®Œæˆ', { lineCount, wordCount })

    return {
      success: true,
      text: `=== CSV æ•°æ® (${lineCount} è¡Œ) ===\n${text}`,
      wordCount
    }
  }

  /**
   * è§£æ PowerPoint æ–‡æ¡£ (.pptx)
   * æ³¨æ„ï¼š.pptx æ˜¯ ZIP æ ¼å¼ï¼ŒåŒ…å« XML æ–‡ä»¶
   */
  private async parsePPTX(file: File): Promise<ParseResult> {
    try {
      const arrayBuffer = await file.arrayBuffer()
      const zip = await this.unzip(arrayBuffer)
      
      const textParts: string[] = []
      let slideCount = 0

      // PPTX çš„å¹»ç¯ç‰‡å†…å®¹åœ¨ ppt/slides/slide*.xml ä¸­
      const slideFiles = Object.keys(zip).filter(name => 
        name.match(/ppt\/slides\/slide\d+\.xml/)
      ).sort((a, b) => {
        const numA = parseInt(a.match(/slide(\d+)/)?.[1] || '0')
        const numB = parseInt(b.match(/slide(\d+)/)?.[1] || '0')
        return numA - numB
      })

      for (const slideFile of slideFiles) {
        slideCount++
        const xmlContent = zip[slideFile]
        const slideText = this.extractTextFromXML(xmlContent)
        if (slideText.trim()) {
          textParts.push(`=== å¹»ç¯ç‰‡ ${slideCount} ===\n${slideText}`)
        }
      }

      const fullText = textParts.join('\n\n')
      const wordCount = this.countWords(fullText)

      logger.info('PowerPoint è§£æå®Œæˆ', { slideCount, wordCount })

      return {
        success: true,
        text: fullText,
        slideCount,
        wordCount
      }
    } catch (error) {
      logger.error('PPTX è§£æå¤±è´¥', error instanceof Error ? error : { error })
      return {
        success: false,
        text: '',
        error: `PowerPoint è§£æå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }
  }

  /**
   * ç®€å•çš„ ZIP è§£å‹ï¼ˆç”¨äº PPTXï¼‰
   */
  private async unzip(arrayBuffer: ArrayBuffer): Promise<Record<string, string>> {
    const zip: Record<string, string> = {}
    
    try {
      const JSZipLib = await loadJSZip() as { loadAsync: (data: Blob) => Promise<{ files: Record<string, { dir: boolean; async: (type: string) => Promise<string> }> }> }
      const uint8Array = new Uint8Array(arrayBuffer)
      const blob = new Blob([uint8Array])
      
      const zipFile = await JSZipLib.loadAsync(blob)
      for (const [name, file] of Object.entries(zipFile.files)) {
        if (!file.dir) {
          try {
            zip[name] = await file.async('string')
          } catch {
            // è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶
          }
        }
      }
    } catch (error) {
      logger.error('ZIP è§£å‹å¤±è´¥', error instanceof Error ? error : { error })
    }
    
    return zip
  }

  /**
   * ä» PPTX XML ä¸­æå–çº¯æ–‡æœ¬
   */
  private extractTextFromXML(xml: string): string {
    // ç§»é™¤ XML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
    // ç‰¹åˆ«å¤„ç† <a:t> æ ‡ç­¾ï¼ˆPowerPoint æ–‡æœ¬æ ‡ç­¾ï¼‰
    const textMatches = xml.match(/<a:t[^>]*>([^<]*)<\/a:t>/g) || []
    const texts = textMatches.map(match => {
      const content = match.replace(/<[^>]+>/g, '')
      return content
    })
    return texts.join(' ')
  }

  /**
   * ä» DOCX XML ä¸­æå–çº¯æ–‡æœ¬
   */
  private extractTextFromDocx(xml: string): string {
    const paragraphs: string[] = []
    
    // åŒ¹é…æ®µè½ <w:p>...</w:p>
    const paragraphRegex = /<w:p[^>]*>([\s\S]*?)<\/w:p>/g
    let paragraphMatch
    
    while ((paragraphMatch = paragraphRegex.exec(xml)) !== null) {
      const paragraphContent = paragraphMatch[1]
      const texts: string[] = []
      
      // åœ¨æ®µè½ä¸­åŒ¹é…æ–‡æœ¬ <w:t>...</w:t> æˆ– <w:t ...>...</w:t>
      const textRegex = /<w:t[^>]*>([^<]*)<\/w:t>/g
      let textMatch
      
      while ((textMatch = textRegex.exec(paragraphContent)) !== null) {
        if (textMatch[1]) {
          texts.push(textMatch[1])
        }
      }
      
      if (texts.length > 0) {
        paragraphs.push(texts.join(''))
      }
    }
    
    return paragraphs.join('\n')
  }

  /**
   * è§£æçº¯æ–‡æœ¬æ–‡ä»¶
   */
  private async parseText(file: File, isCode: boolean): Promise<ParseResult> {
    const text = await file.text()
    const wordCount = this.countWords(text)
    const lineCount = text.split('\n').length

    logger.info('æ–‡æœ¬è§£æå®Œæˆ', { wordCount, lineCount, isCode })

    // å¦‚æœæ˜¯ä»£ç æ–‡ä»¶ï¼Œæ·»åŠ è¯­è¨€æ ‡è¯†
    let formattedText = text
    if (isCode) {
      const ext = this.getExtension(file.name)
      const lang = this.getLanguageFromExt(ext)
      formattedText = `\`\`\`${lang}\n${text}\n\`\`\``
    }

    return {
      success: true,
      text: formattedText,
      wordCount
    }
  }

  /**
   * æ ¹æ®æ‰©å±•åè·å–ç¼–ç¨‹è¯­è¨€
   */
  private getLanguageFromExt(ext: string): string {
    const langMap: Record<string, string> = {
      '.js': 'javascript',
      '.ts': 'typescript',
      '.jsx': 'jsx',
      '.tsx': 'tsx',
      '.py': 'python',
      '.java': 'java',
      '.kt': 'kotlin',
      '.go': 'go',
      '.rs': 'rust',
      '.c': 'c',
      '.cpp': 'cpp',
      '.h': 'c',
      '.hpp': 'cpp',
      '.cs': 'csharp',
      '.rb': 'ruby',
      '.php': 'php',
      '.swift': 'swift',
      '.sql': 'sql',
      '.sh': 'bash',
      '.bash': 'bash',
      '.ps1': 'powershell',
      '.r': 'r',
      '.R': 'r'
    }
    return langMap[ext] || ext.slice(1)
  }

  /**
   * è·å–æ–‡ä»¶æ‰©å±•å
   */
  private getExtension(fileName: string): string {
    const lastDot = fileName.lastIndexOf('.')
    return lastDot >= 0 ? fileName.slice(lastDot).toLowerCase() : ''
  }

  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶
   */
  private isTextFile(ext: string): boolean {
    return TEXT_EXTENSIONS.includes(ext)
  }

  /**
   * åˆ¤æ–­æ‰©å±•åæ˜¯å¦æ”¯æŒ
   */
  private isSupportedExtension(ext: string): boolean {
    const docExtensions = ['.pdf', '.doc', '.docx', '.xlsx', '.xls', '.csv', '.pptx']
    return docExtensions.includes(ext) || this.isTextFile(ext)
  }

  /**
   * ç»Ÿè®¡å­—æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰
   */
  private countWords(text: string): number {
    const cleanText = text.replace(/\s+/g, ' ').trim()
    if (!cleanText) return 0

    const chineseChars = (cleanText.match(/[\u4e00-\u9fa5]/g) || []).length
    const englishWords = cleanText
      .replace(/[\u4e00-\u9fa5]/g, ' ')
      .split(/\s+/)
      .filter(word => word.length > 0).length

    return chineseChars + englishWords
  }

  /**
   * æˆªæ–­æ–‡æœ¬ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
   */
  truncateText(text: string, maxLength: number = 50000): string {
    if (text.length <= maxLength) return text

    const truncated = text.substring(0, maxLength)
    return truncated + '\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­ï¼ŒåŸæ–‡å…± ' + text.length + ' å­—ç¬¦]'
  }

  /**
   * ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–å†…å®¹
   */
  extractSections(text: string): DocumentSection[] {
    const sections: DocumentSection[] = []
    const lines = text.split('\n')
    let currentIndex = 0

    for (const line of lines) {
      const trimmedLine = line.trim()
      if (!trimmedLine) {
        currentIndex += line.length + 1
        continue
      }

      // æ£€æµ‹ Markdown æ ‡é¢˜
      const headingMatch = trimmedLine.match(/^(#{1,6})\s+(.+)$/)
      if (headingMatch) {
        sections.push({
          type: 'heading',
          level: headingMatch[1].length,
          content: headingMatch[2],
          startIndex: currentIndex
        })
        currentIndex += line.length + 1
        continue
      }

      // æ£€æµ‹ä¸­æ–‡æ•°å­—æ ‡é¢˜ï¼ˆå¦‚ï¼šä¸€ã€äºŒã€ä¸‰ï¼‰
      const chineseHeadingMatch = trimmedLine.match(/^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.ï¼]\s*(.+)$/)
      if (chineseHeadingMatch) {
        sections.push({
          type: 'heading',
          level: 1,
          content: trimmedLine,
          startIndex: currentIndex
        })
        currentIndex += line.length + 1
        continue
      }

      // æ£€æµ‹é˜¿æ‹‰ä¼¯æ•°å­—æ ‡é¢˜ï¼ˆå¦‚ï¼š1. 2. æˆ– 1.1ï¼‰
      const numberHeadingMatch = trimmedLine.match(/^(\d+\.)+\s*(.+)$/)
      if (numberHeadingMatch && trimmedLine.length < 100) {
        const level = (numberHeadingMatch[1].match(/\./g) || []).length
        sections.push({
          type: 'heading',
          level: Math.min(level, 6),
          content: trimmedLine,
          startIndex: currentIndex
        })
        currentIndex += line.length + 1
        continue
      }

      // æ£€æµ‹åˆ—è¡¨é¡¹
      if (/^[-*â€¢]\s+/.test(trimmedLine) || /^\d+[.)]\s+/.test(trimmedLine)) {
        sections.push({
          type: 'list',
          content: trimmedLine,
          startIndex: currentIndex
        })
        currentIndex += line.length + 1
        continue
      }

      // æ£€æµ‹ä»£ç å—
      if (trimmedLine.startsWith('```')) {
        sections.push({
          type: 'code',
          content: trimmedLine,
          startIndex: currentIndex
        })
        currentIndex += line.length + 1
        continue
      }

      // æ™®é€šæ®µè½
      if (trimmedLine.length > 0) {
        sections.push({
          type: 'paragraph',
          content: trimmedLine,
          startIndex: currentIndex
        })
      }

      currentIndex += line.length + 1
    }

    return sections
  }

  /**
   * ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
   */
  generateSummary(text: string, sections: DocumentSection[]): DocumentSummary {
    // æå–ç›®å½•ï¼ˆæ ‡é¢˜åˆ—è¡¨ï¼‰
    const tableOfContents = sections
      .filter(s => s.type === 'heading')
      .map(s => {
        const indent = '  '.repeat((s.level || 1) - 1)
        return `${indent}${s.content}`
      })

    // æå–å…³é”®ç‚¹ï¼ˆå‰å‡ ä¸ªæ®µè½æˆ–åˆ—è¡¨é¡¹ï¼‰
    const keyPoints: string[] = []
    const headings = sections.filter(s => s.type === 'heading')
    
    // ä»æ ‡é¢˜æå–å…³é”®ç‚¹
    headings.slice(0, 5).forEach(h => {
      if (h.content.length < 50) {
        keyPoints.push(h.content)
      }
    })

    // ç”Ÿæˆç®€çŸ­æ‘˜è¦
    let brief = ''
    const firstParagraph = sections.find(s => s.type === 'paragraph')
    if (firstParagraph) {
      brief = firstParagraph.content.slice(0, 100)
      if (firstParagraph.content.length > 100) {
        brief += '...'
      }
    } else if (headings.length > 0) {
      brief = `æœ¬æ–‡æ¡£åŒ…å« ${headings.length} ä¸ªç« èŠ‚`
      if (headings[0]) {
        brief += `ï¼Œé¦–ç« : ${headings[0].content}`
      }
    } else {
      brief = text.slice(0, 100) + (text.length > 100 ? '...' : '')
    }

    return {
      brief,
      keyPoints,
      tableOfContents: tableOfContents.length > 0 ? tableOfContents : undefined
    }
  }

  /**
   * ä» DOCX XML æå–å…ƒæ•°æ®
   */
  private extractDocxMetadata(coreXml: string | undefined): DocumentMetadata | undefined {
    if (!coreXml) return undefined

    const metadata: DocumentMetadata = {}

    // æå–æ ‡é¢˜
    const titleMatch = coreXml.match(/<dc:title>([^<]*)<\/dc:title>/)
    if (titleMatch) metadata.title = titleMatch[1]

    // æå–ä½œè€…
    const authorMatch = coreXml.match(/<dc:creator>([^<]*)<\/dc:creator>/)
    if (authorMatch) metadata.author = authorMatch[1]

    // æå–ä¸»é¢˜
    const subjectMatch = coreXml.match(/<dc:subject>([^<]*)<\/dc:subject>/)
    if (subjectMatch) metadata.subject = subjectMatch[1]

    // æå–å…³é”®è¯
    const keywordsMatch = coreXml.match(/<cp:keywords>([^<]*)<\/cp:keywords>/)
    if (keywordsMatch) {
      metadata.keywords = keywordsMatch[1].split(/[,;ï¼Œï¼›]/).map(k => k.trim()).filter(Boolean)
    }

    // æå–åˆ›å»ºæ—¶é—´
    const createdMatch = coreXml.match(/<dcterms:created[^>]*>([^<]*)<\/dcterms:created>/)
    if (createdMatch) metadata.createdAt = createdMatch[1]

    // æå–ä¿®æ”¹æ—¶é—´
    const modifiedMatch = coreXml.match(/<dcterms:modified[^>]*>([^<]*)<\/dcterms:modified>/)
    if (modifiedMatch) metadata.modifiedAt = modifiedMatch[1]

    return Object.keys(metadata).length > 0 ? metadata : undefined
  }

  /**
   * æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹ç”¨äº AI ä¸Šä¸‹æ–‡
   * åŒ…å«ç»“æ„åŒ–ä¿¡æ¯å’Œæ‘˜è¦
   */
  formatForAIContext(parseResult: ParseResult, fileName: string): string {
    const parts: string[] = []

    // æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    parts.push(`ğŸ“„ æ–‡ä»¶: ${fileName}`)
    
    if (parseResult.wordCount) {
      parts.push(`ğŸ“Š å­—æ•°: ${parseResult.wordCount}`)
    }
    if (parseResult.pageCount) {
      parts.push(`ğŸ“„ é¡µæ•°: ${parseResult.pageCount}`)
    }
    if (parseResult.sheetCount) {
      parts.push(`ğŸ“‹ å·¥ä½œè¡¨: ${parseResult.sheetCount}`)
    }
    if (parseResult.slideCount) {
      parts.push(`ğŸ¯ å¹»ç¯ç‰‡: ${parseResult.slideCount}`)
    }

    // å…ƒæ•°æ®
    if (parseResult.metadata) {
      const meta = parseResult.metadata
      if (meta.title) parts.push(`ğŸ“Œ æ ‡é¢˜: ${meta.title}`)
      if (meta.author) parts.push(`ğŸ‘¤ ä½œè€…: ${meta.author}`)
    }

    // æ‘˜è¦
    if (parseResult.summary) {
      parts.push('')
      parts.push('ã€æ–‡æ¡£æ‘˜è¦ã€‘')
      parts.push(parseResult.summary.brief)
      
      if (parseResult.summary.tableOfContents && parseResult.summary.tableOfContents.length > 0) {
        parts.push('')
        parts.push('ã€ç›®å½•ç»“æ„ã€‘')
        parts.push(parseResult.summary.tableOfContents.slice(0, 10).join('\n'))
        if (parseResult.summary.tableOfContents.length > 10) {
          parts.push(`... è¿˜æœ‰ ${parseResult.summary.tableOfContents.length - 10} ä¸ªç« èŠ‚`)
        }
      }
    }

    // å®Œæ•´å†…å®¹
    parts.push('')
    parts.push('ã€å®Œæ•´å†…å®¹ã€‘')
    parts.push(parseResult.text)

    return parts.join('\n')
  }
}

// å•ä¾‹å¯¼å‡º
export const DocumentParser = new DocumentParserClass()
